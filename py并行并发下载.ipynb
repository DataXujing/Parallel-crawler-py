{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Py爬虫并发并行下载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "徐静"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 100万个网站"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "亚马逊子公司Alexa提供了最受欢迎的100万个网站列表（http://www.alexa.com/topsites ），我们也可以通过http://s3.amazonaws.com/alexa-static/top-1m.csv.zip 直接下载这一列表的压缩文件，这样就不用去提取Alexa网站的数据了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>google.com</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>facebook.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>baidu.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>wikipedia.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>yahoo.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1     google.com\n",
       "0  2    youtube.com\n",
       "1  3   facebook.com\n",
       "2  4      baidu.com\n",
       "3  5  wikipedia.org\n",
       "4  6      yahoo.com"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv(\"data/top-1m.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  用普通方法解析Alexa列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取数据的4个步骤： \n",
    "\n",
    "> 下载.zip文件； \n",
    "\n",
    "> 从.zip文件中提取出CSV文件；\n",
    "\n",
    "> 解析CSV文件； \n",
    "\n",
    "> 遍历CSV文件中的每一行，从中提取出域名数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "from StringIO import StringIO\n",
    "from downloader import Downloader\n",
    "\n",
    "def alexa():\n",
    "    D = Downloader()\n",
    "    zipped_data = D('http://s3.amazonaws.com/alexa-static/top-1m.csv.zip')\n",
    "    urls = [] # top 1 million URL's will be stored in this list\n",
    "    with ZipFile(StringIO(zipped_data)) as zf:\n",
    "        csv_filename = zf.namelist()[0]\n",
    "        for _, website in csv.reader(zf.open(csv_filename)):\n",
    "            urls.append('http://' + website)\n",
    "    return urls\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    print (len(alexa()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载得到的压缩数据是使用StringIO封装之后，才传给ZipFile，是因为ZipFile需要一个相关的接口，而不是字符串。由于这个zip文件只包含一个文件，所以直接选择第一个文件即可。然后在域名数据前添加http://协议，附加到URL列表中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 复用爬虫代码解析Alexa列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要复用上述功能，需要修改scrape_callback接口。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "from StringIO import StringIO\n",
    "from mongo_cache import MongoCache\n",
    "\n",
    "class AlexaCallback:\n",
    "    def __init__(self, max_urls=1000):\n",
    "        self.max_urls = max_urls\n",
    "        self.seed_url = 'http://s3.amazonaws.com/alexa-static/top-1m.csv.zip'\n",
    "\n",
    "    def __call__(self, url, html):\n",
    "        if url == self.seed_url:\n",
    "            urls = []\n",
    "            #cache = MongoCache()\n",
    "            with ZipFile(StringIO(html)) as zf:\n",
    "                csv_filename = zf.namelist()[0]\n",
    "                for _, website in csv.reader(zf.open(csv_filename)):\n",
    "                    if 'http://' + website not in cache:\n",
    "                        urls.append('http://' + website)\n",
    "                        if len(urls) == self.max_urls:\n",
    "                            break\n",
    "            return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里添加了一个新的输入参数max_urls，用于设定从Alexa文件中提取的URL数量。如果真要下载100万个网页，那要消耗11天的时间，所以这里只设置为1000个URL。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  串行爬虫"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from link_crawler import link_crawler\n",
    "from mongo_cache import MongoCache\n",
    "from alexa_cb import AlexaCallback\n",
    "\n",
    "def main():\n",
    "    scrape_callback = AlexaCallback()\n",
    "    cache = MongoCache()\n",
    "    #cache.clear()\n",
    "    link_crawler(scrape_callback.seed_url, scrape_callback=scrape_callback, cache=cache, timeout=10, ignore_robots=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 并发并行爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了加快下载网页速度，我们用多进程和多线程将串行下载扩展成并发下载，并将delay标识最小时间间隔为1秒，以免造成服务器过载，或导致IP地址封禁。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 并发并行工作原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "并行是基于多处理器多核而言的，让多个处理器多核真正同时跑多个程序或多个进程。而并发是单个处理器而言的，同一时刻每个处理器只会执行一个进程，然后在不同进程间快速切换，宏观上给人以多个程序同时运行的感觉，但微观上单个处理器还是串行工作的。同理，在一个进程中，程序的执行也是不同线程间进行切换的，每个线程执行程序的的不同部分。这就意味着当一个线程等待网页下载时，进程可以切换到其他线程执行，避免浪费处理器时间。因此，为了充分利用计算机中的所有资源尽可能快地下载数据，我们需要将下载分发到多个进程和线程中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多线程爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以修改第一篇文章链接爬虫队列结构的代码，修改为多个线程中启动爬虫循环process_queue()，以便并发下载这些链接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import urlparse\n",
    "from downloader import Downloader\n",
    "\n",
    "SLEEP_TIME = 1\n",
    "\n",
    "def threaded_crawler(seed_url, delay=5, cache=None, scrape_callback=None, user_agent='Wu_Being', proxies=None, num_retries=1, max_threads=10, timeout=60):\n",
    "    \"\"\"Crawl this website in multiple threads\n",
    "    \"\"\"\n",
    "    # the queue of URL's that still need to be crawled\n",
    "    #crawl_queue = Queue.deque([seed_url])\n",
    "    crawl_queue = [seed_url]\n",
    "    # the URL's that have been seen \n",
    "    seen = set([seed_url])\n",
    "    D = Downloader(cache=cache, delay=delay, user_agent=user_agent, proxies=proxies, num_retries=num_retries, timeout=timeout)\n",
    "\n",
    "    def process_queue():\n",
    "        while True:\n",
    "            try:\n",
    "                url = crawl_queue.pop()\n",
    "            except IndexError:\n",
    "                # crawl queue is empty\n",
    "                break\n",
    "            else:\n",
    "                html = D(url)\n",
    "                if scrape_callback:\n",
    "                    try:\n",
    "                        links = scrape_callback(url, html) or []\n",
    "                    except Exception as e:\n",
    "                        print 'Error in callback for: {}: {}'.format(url, e)\n",
    "                    else:\n",
    "                        for link in links:\n",
    "                            link = normalize(seed_url, link)\n",
    "                            # check whether already crawled this link\n",
    "                            if link not in seen:\n",
    "                                seen.add(link)\n",
    "                                # add this new link to queue\n",
    "                                crawl_queue.append(link)\n",
    "\n",
    "    # wait for all download threads to finish\n",
    "    threads = []\n",
    "    while threads or crawl_queue:\n",
    "        # the crawl is still active\n",
    "        for thread in threads:\n",
    "            if not thread.is_alive():\n",
    "                # remove the stopped threads\n",
    "                threads.remove(thread)\n",
    "        while len(threads) < max_threads and crawl_queue:\n",
    "            # can start some more threads\n",
    "            thread = threading.Thread(target=process_queue)\n",
    "            thread.setDaemon(True) # set daemon so main thread can exit when receives ctrl-c\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "        # all threads have been processed\n",
    "        # sleep temporarily so CPU can focus execution on other threads\n",
    "        time.sleep(SLEEP_TIME)\n",
    "\n",
    "def normalize(seed_url, link):\n",
    "    \"\"\"Normalize this URL by removing hash and adding domain\n",
    "    \"\"\"\n",
    "    link, _ = urlparse.urldefrag(link) # remove hash to avoid duplicates\n",
    "    return urlparse.urljoin(seed_url, link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面代码在循环会不断创建线程，直到达到线程池threads的最大值。在爬取过程中，如果当前列队没有更多可以爬取的URL时，该线程会提前停止。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如当前有两个线程以及两个待下载的URL，当第一个线程完成下载时，待爬取队列为空，则该线程退出。第二个线程稍后也完成了下载，但又发现了另一个待下载的URL"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "from threaded_crawler import threaded_crawler\n",
    "from mongo_cache import MongoCache\n",
    "from alexa_cb import AlexaCallback\n",
    "\n",
    "def main(max_threads):\n",
    "    scrape_callback = AlexaCallback()\n",
    "    cache = MongoCache()\n",
    "    #cache.clear()\n",
    "    threaded_crawler(scrape_callback.seed_url, scrape_callback=scrape_callback, cache=cache, max_threads=max_threads, timeout=10)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    max_threads = int(sys.argv[1])\n",
    "    main(max_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面使用了5个线程，因此下载速度几乎是串行版本的5倍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多进程爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于有多核的中央处理器，则可以启动多进程。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "from process_crawler import process_crawler\n",
    "from mongo_cache import MongoCache\n",
    "from alexa_cb import AlexaCallback\n",
    "\n",
    "def main(max_threads):\n",
    "    scrape_callback = AlexaCallback()\n",
    "    cache = MongoCache()\n",
    "    cache.clear()\n",
    "    process_crawler(scrape_callback.seed_url, scrape_callback=scrape_callback, cache=cache, max_threads=max_threads, timeout=10) ##process_crawler\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    max_threads = int(sys.argv[1])\n",
    "    main(max_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面代码首先获取中央处理器内核个数，然后启动相应的进程个数，在每进程启动多个线程爬虫。之前的爬虫队列是存储在本地内存中，其他进程都无法处理这一爬虫，为了解决这一问题，需要把爬虫队列转移到MongoDB当中。单独存储队列，意味着即使是不同服务器上的爬虫也能够协同处理同一个爬虫任务。我们可以使用更加健壮的队列，比如专用的消息传输工具Celery，这里我们利用mongodb实现的队列代码。在threaded_crawler需要做如下修改： \n",
    "- 内建的队列换成基于MongoDB的新队列MongoQueue； \n",
    "- 由于队列内部实现中处理重复URL的问题，因此不再需要seen变量； \n",
    "- 在URL处理结束后调用complete()方法，用于记录该URL已经被成功解析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import urlparse\n",
    "import threading\n",
    "import multiprocessing\n",
    "from mongo_cache import MongoCache\n",
    "from mongo_queue import MongoQueue\n",
    "from downloader import Downloader\n",
    "\n",
    "SLEEP_TIME = 1\n",
    "\n",
    "### process_crawler(scrape_callback.seed_url, scrape_callback=scrape_callback, cache=cache, max_threads=max_threads, timeout=10)\n",
    "def process_crawler(args, **kwargs):    #args:number of args, kwargs:args list\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    #pool = multiprocessing.Pool(processes=num_cpus)\n",
    "    print 'Starting {} processes...'.format(num_cpus)   ######################\n",
    "    processes = []\n",
    "    for i in range(num_cpus):\n",
    "        p = multiprocessing.Process(target=threaded_crawler, args=[args], kwargs=kwargs)### threaded_crawler\n",
    "        #parsed = pool.apply_async(threaded_link_crawler, args, kwargs)\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    # wait for processes to complete\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "def threaded_crawler(seed_url, delay=5, cache=None, scrape_callback=None, user_agent='wu_being', proxies=None, num_retries=1, max_threads=10, timeout=60):\n",
    "    \"\"\"Crawl using multiple threads\n",
    "    \"\"\"\n",
    "    # the queue of URL's that still need to be crawled\n",
    "    crawl_queue = MongoQueue()  ######################\n",
    "    crawl_queue.clear()     ######################\n",
    "    crawl_queue.push(seed_url)  ######################\n",
    "    D = Downloader(cache=cache, delay=delay, user_agent=user_agent, proxies=proxies, num_retries=num_retries, timeout=timeout)\n",
    "\n",
    "    def process_queue():\n",
    "        while True:\n",
    "            # keep track that are processing url\n",
    "            try:\n",
    "                url = crawl_queue.pop() ######################\n",
    "            except KeyError:\n",
    "                # currently no urls to process\n",
    "                break\n",
    "            else:\n",
    "                html = D(url)\n",
    "                if scrape_callback:\n",
    "                    try:\n",
    "                        links = scrape_callback(url, html) or []\n",
    "                    except Exception as e:\n",
    "                        print 'Error in callback for: {}: {}'.format(url, e)\n",
    "                    else:\n",
    "                        for link in links:      #############\n",
    "                            # add this new link to queue######################\n",
    "                            crawl_queue.push(normalize(seed_url, link))######################\n",
    "                crawl_queue.complete(url)       ######################\n",
    "\n",
    "    # wait for all download threads to finish\n",
    "    threads = []\n",
    "    while threads or crawl_queue:           ######################\n",
    "        for thread in threads:\n",
    "            if not thread.is_alive():\n",
    "                threads.remove(thread)\n",
    "        while len(threads) < max_threads and crawl_queue.peek():    #######################\n",
    "            # can start some more threads\n",
    "            thread = threading.Thread(target=process_queue)\n",
    "            thread.setDaemon(True) # set daemon so main thread can exit when receives ctrl-c\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "        time.sleep(SLEEP_TIME)\n",
    "\n",
    "def normalize(seed_url, link):\n",
    "    \"\"\"Normalize this URL by removing hash and adding domain\n",
    "    \"\"\"\n",
    "    link, _ = urlparse.urldefrag(link) # remove hash to avoid duplicates\n",
    "    return urlparse.urljoin(seed_url, link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoQueue定义了三种状态： \n",
    "- OUTSTANDING：添加一人新URL时； \n",
    "- PROCESSING：队列中取出准备下载时； \n",
    "- COMPLETE：完成下载时。\n",
    "\n",
    "由于大部分线程都在从队列准备取出未完成处理的URL，比如处理的URL线程被终止的情况。所以在该类中使用了timeout参数，默认为300秒。在repaire()方法中，如果某个URL的处理时间超过了这个timeout值，我们就认定处理过程出现了错误，URL的状态将被重新设为OUTSTANDING，以便再次处理。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pymongo import MongoClient, errors\n",
    "\n",
    "class MongoQueue:\n",
    "    \"\"\"\n",
    "    >>> timeout = 1\n",
    "    >>> url = 'http://example.webscraping.com'\n",
    "    >>> q = MongoQueue(timeout=timeout)\n",
    "    >>> q.clear() # ensure empty queue\n",
    "    >>> q.push(url) # add test URL\n",
    "    >>> q.peek() == q.pop() == url # pop back this URL\n",
    "    True\n",
    "    >>> q.repair() # immediate repair will do nothin\n",
    "    >>> q.pop() # another pop should be empty\n",
    "    >>> q.peek() \n",
    "    >>> import time; time.sleep(timeout) # wait for timeout\n",
    "    >>> q.repair() # now repair will release URL\n",
    "    Released: test\n",
    "    >>> q.pop() == url # pop URL again\n",
    "    True\n",
    "    >>> bool(q) # queue is still active while outstanding\n",
    "    True\n",
    "    >>> q.complete(url) # complete this URL\n",
    "    >>> bool(q) # queue is not complete\n",
    "    False\n",
    "    \"\"\"\n",
    "\n",
    "    # possible states of a download\n",
    "    OUTSTANDING, PROCESSING, COMPLETE = range(3)\n",
    "\n",
    "    def __init__(self, client=None, timeout=300):\n",
    "        \"\"\"\n",
    "        host: the host to connect to MongoDB\n",
    "        port: the port to connect to MongoDB\n",
    "        timeout: the number of seconds to allow for a timeout\n",
    "        \"\"\"\n",
    "        self.client = MongoClient() if client is None else client\n",
    "        self.db = self.client.cache\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def __nonzero__(self):\n",
    "        \"\"\"Returns True if there are more jobs to process\n",
    "        \"\"\"\n",
    "        record = self.db.crawl_queue.find_one(\n",
    "            {'status': {'$ne': self.COMPLETE}} \n",
    "        )\n",
    "        return True if record else False\n",
    "\n",
    "    def push(self, url):\n",
    "        \"\"\"Add new URL to queue if does not exist\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.db.crawl_queue.insert({'_id': url, 'status': self.OUTSTANDING})\n",
    "        except errors.DuplicateKeyError as e:\n",
    "            pass # this is already in the queue\n",
    "\n",
    "    def pop(self):\n",
    "        \"\"\"Get an outstanding URL from the queue and set its status to processing.\n",
    "        If the queue is empty a KeyError exception is raised.\n",
    "        \"\"\"\n",
    "        record = self.db.crawl_queue.find_and_modify(\n",
    "            query={'status': self.OUTSTANDING}, \n",
    "            update={'$set': {'status': self.PROCESSING, 'timestamp': datetime.now()}}\n",
    "        )\n",
    "        if record:\n",
    "            return record['_id']\n",
    "        else:\n",
    "            self.repair()\n",
    "            raise KeyError()\n",
    "\n",
    "    def peek(self):\n",
    "        record = self.db.crawl_queue.find_one({'status': self.OUTSTANDING})\n",
    "        if record:\n",
    "            return record['_id']\n",
    "\n",
    "    def complete(self, url):\n",
    "        self.db.crawl_queue.update({'_id': url}, {'$set': {'status': self.COMPLETE}})\n",
    "\n",
    "    def repair(self):\n",
    "        \"\"\"Release stalled jobs\n",
    "        \"\"\"\n",
    "        record = self.db.crawl_queue.find_and_modify(\n",
    "            query={\n",
    "                'timestamp': {'$lt': datetime.now() - timedelta(seconds=self.timeout)},\n",
    "                'status': {'$ne': self.COMPLETE}\n",
    "            },\n",
    "            update={'$set': {'status': self.OUTSTANDING}}\n",
    "        )\n",
    "        if record:\n",
    "            print 'Released:', record['_id']\n",
    "\n",
    "    def clear(self):\n",
    "        self.db.crawl_queue.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，下载的带宽是有限的，最终添加新线程将无法加快的下载速度。因此要想获得更好性能的爬虫，就需要在多台服务器上分布式部署爬虫，并且所有服务器都要指向同一个MongoDB队列实例中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python爬虫系列的GitHub代码文件：https://github.com/1040003585/WebScrapingWithPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style=\"text-align:center;\">2017© 徐静<p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
